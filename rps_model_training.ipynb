{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# System Installation / Preparation (macOS Homebrew)\n",
    "brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Pip Installation / Preparation\n",
    "pip install pydot\n",
    "pip install tensorflow\n",
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import pydot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as pandas dataframe\n",
    "df = pd.read_csv(\"./data.csv\")\n",
    "\n",
    "# Print: first records\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimension of input data frame\n",
    "num_rows, num_cols = df.shape\n",
    "print(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the record sets from the input data frame\n",
    "record_sets = []\n",
    "set_start_index = 0\n",
    "set_end_index = 2\n",
    "for index in range(int(num_cols/2)):\n",
    "    record_sets.append(df.iloc[0:,set_start_index:set_end_index])\n",
    "    set_start_index = set_start_index + 2\n",
    "    set_end_index = set_end_index + 2\n",
    "\n",
    "# Print: first record set\n",
    "record_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the winner for each record in all record sets\n",
    "wins = []\n",
    "for index in range(len(record_sets)):\n",
    "    winner_arr = []\n",
    "    record_set = record_sets[index]\n",
    "    for row in range(num_rows):\n",
    "        #print(record_set.iloc[row,0], record_set.iloc[row,1])\n",
    "        if record_set.iloc[row,1] == record_set.iloc[row,0]:\n",
    "            winner_arr.append(0)                                # tie\n",
    "        elif record_set.iloc[row,1] == 'R':                     # player = record_set[row,1]\n",
    "            if record_set.iloc[row,0] == 'P':                   # computer = record_set[row,0]\n",
    "                winner_arr.append(-1)                           # player lose\n",
    "            if record_set.iloc[row,0] == 'S':\n",
    "                winner_arr.append(1)                            # player win\n",
    "        elif record_set.iloc[row,1] == 'S':\n",
    "            if record_set.iloc[row,0] == 'R':\n",
    "                winner_arr.append(-1)\n",
    "            if record_set.iloc[row,0] == 'P':\n",
    "                winner_arr.append(1)\n",
    "        elif record_set.iloc[row,1] == 'P':\n",
    "            if record_set.iloc[row,0] == 'S':\n",
    "                winner_arr.append(-1)\n",
    "            if record_set.iloc[row,0] == 'R':\n",
    "                winner_arr.append(1)\n",
    "    wins.append(winner_arr)\n",
    "\n",
    "# Print: wins array of arrays ( = 2D array)\n",
    "print(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wins array to Pandas data frame\n",
    "win_dfs = []\n",
    "for index in range(len(wins)):\n",
    "    header = 'W' + str(index + 1)\n",
    "    win_df = pd.DataFrame(wins[index], columns=[header])\n",
    "    win_dfs.append(win_df)\n",
    "\n",
    "# Print: first data frame\n",
    "win_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OneHotEncoder categories of the input data frame\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(df.to_numpy().flatten().reshape(-1, 1))\n",
    "\n",
    "# Print: array of OneHotEncoder categories\n",
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample one hot encoding by the created OneHotEncoder\n",
    "sample = enc.transform(np.array('R').reshape(-1, 1))\n",
    "sample.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the input data frame\n",
    "one_hot_df = df.applymap(lambda x: enc.transform(np.array(x).reshape(-1, 1)).toarray()[0])\n",
    "\n",
    "# Print: one hot encoded input data frame\n",
    "one_hot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the one hot encoded record sets from the input data frame\n",
    "one_hot_record_sets = []\n",
    "one_hot_set_start_index = 0\n",
    "one_hot_set_end_index = 2\n",
    "for index in range(int(num_cols/2)):\n",
    "    one_hot_record_sets.append(one_hot_df.iloc[0:,one_hot_set_start_index:one_hot_set_end_index])\n",
    "    one_hot_set_start_index = one_hot_set_start_index + 2\n",
    "    one_hot_set_end_index = one_hot_set_end_index + 2\n",
    "\n",
    "# Print: first one hot encoded record set\n",
    "one_hot_record_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine/Concat one hot-encoded record sets with the wins data frames\n",
    "combined_record_sets = []\n",
    "for index in range(len(one_hot_record_sets)):\n",
    "    stacked = pd.concat([one_hot_record_sets[index], win_dfs[index]], axis=1)\n",
    "    combined_record_sets.append(stacked)\n",
    "\n",
    "# Print: first combined record set\n",
    "combined_record_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows for the training and validation data\n",
    "training_rows = round(num_rows * 0.7)\n",
    "print(training_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split combined record sets into training and validation data\n",
    "training_record_sets = []\n",
    "validation_record_sets = []\n",
    "for index in range(len(combined_record_sets)):\n",
    "    record_set = combined_record_sets[index]\n",
    "    # split\n",
    "    training_set = record_set.iloc[:training_rows]\n",
    "    validation_set = record_set.iloc[training_rows:]\n",
    "    # append\n",
    "    training_record_sets.append(training_set)     \n",
    "    validation_record_sets.append(validation_set)       \n",
    "\n",
    "# Print: first training record set\n",
    "training_record_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename headers for merging\n",
    "renamed_training_sets = []\n",
    "renamed_validation_sets = []\n",
    "# Training data sets\n",
    "for index in range(len(training_record_sets)):\n",
    "    record_set = training_record_sets[index]\n",
    "    header_com = 'C' + str(index + 1)\n",
    "    header_human = 'H' + str(index + 1)\n",
    "    header_win = 'W' + str(index + 1)\n",
    "    renamed_training_sets.append(record_set.rename(columns={header_com: \"C\", header_human: \"H\", header_win: \"W\"}))\n",
    "# Validation data sets\n",
    "for index in range(len(validation_record_sets)):\n",
    "    record_set = validation_record_sets[index]\n",
    "    header_com = 'C' + str(index + 1)\n",
    "    header_human = 'H' + str(index + 1)\n",
    "    header_win = 'W' + str(index + 1)\n",
    "    renamed_validation_sets.append(record_set.rename(columns={header_com: \"C\", header_human: \"H\", header_win: \"W\"}))\n",
    "\n",
    "# Print: first renamed validation sets\n",
    "renamed_validation_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge training and valitation record sets into one training and one validation data frame\n",
    "# Create training and validation data drame\n",
    "x_training_df = pd.DataFrame(columns=['C', 'H', 'W'])\n",
    "y_training_df = pd.DataFrame(columns=['C', 'H', 'W'])\n",
    "\n",
    "x_validation_df = pd.DataFrame(columns=['C', 'H', 'W'])\n",
    "y_validation_df = pd.DataFrame(columns=['C', 'H', 'W'])\n",
    "# Training\n",
    "for index in range(len(renamed_training_sets)):\n",
    "    training_set = renamed_training_sets[index]\n",
    "    validation_set = renamed_validation_sets[index]\n",
    "    # x\n",
    "    x_training_df = pd.concat([x_training_df, training_set], axis=0)\n",
    "    # y\n",
    "    y_training_df = pd.concat([y_training_df, training_set.iloc[1:,0:]], axis=0)\n",
    "    y_training_df = pd.concat([y_training_df, validation_set.iloc[0:1,0:]], axis=0)\n",
    "# Valitation\n",
    "for index in range(len(renamed_validation_sets)):\n",
    "    validation_set = renamed_validation_sets[index]\n",
    "    # x\n",
    "    x_validation_df = pd.concat([x_validation_df, validation_set.iloc[:-1,0:]], axis=0)\n",
    "    # y\n",
    "    y_validation_df = pd.concat([y_validation_df, validation_set.iloc[1:,0:]], axis=0)\n",
    "\n",
    "# Print: training data frame\n",
    "x_training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layers\n",
    "one_hot_input = keras.Input(shape=(3,), name='one_hot_input')\n",
    "winner_input = keras.Input(shape=(1,), name=\"winner_input\")\n",
    "\n",
    "# Define machine learning model\n",
    "inputs = keras.layers.Concatenate()([one_hot_input, winner_input])\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\", name=\"predictions\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create machine learning model\n",
    "model = keras.Model(inputs=[one_hot_input, winner_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile machine learning model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.Adam(),\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot machine learning model\n",
    "keras.utils.plot_model(model, \"RPS.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare current training data tensors -> T(n)\n",
    "x_train = x_training_df.iloc[0:,1:]\n",
    "x_train_one_hot = tf.constant(np.array(x_train['H'].tolist()))\n",
    "x_train_winner = tf.constant(x_train['W'].astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare following training data tensors -> T(n+1)\n",
    "y_train = y_training_df.iloc[0:,1:2]\n",
    "y_train = tf.constant(np.array(y_train['H'].tolist()))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare current validation data tensors -> V(n)\n",
    "x_val = x_validation_df.iloc[0:,1:]\n",
    "x_val_one_hot = tf.constant(np.array(x_val['H'].tolist()))\n",
    "x_val_winner = tf.constant(x_val['W'].astype('int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare following validation data tensors -> V(n+1)\n",
    "y_val = y_validation_df.iloc[0:,1:2]\n",
    "y_val = tf.constant(np.array(y_val['H'].tolist()))\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    {'one_hot_input': x_train_one_hot, 'winner_input': x_train_winner},    # letztes T, letzter Siegwert, T(n) + W(n)\n",
    "    y_train,    # neues T, T(n+1)\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=({'one_hot_input': x_val_one_hot, 'winner_input': x_val_winner}, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"./rps_model.keras\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCRsaurusRex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
